{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# change working directory to make src visible\n",
    "os.chdir(Path.cwd().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.visualization import draw_event\n",
    "from src.dataset import SPDEventsDataset\n",
    "from src.data_generation import SPDEventGenerator\n",
    "from src.normalization import ConstraintsNormalizer, TrackParamsNormalizer\n",
    "from src.dataset import collate_fn_for_set_loss\n",
    "from src.model import TRT\n",
    "\n",
    "seed_everything(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare single batch for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVENT_TRACKS = 10\n",
    "TRUNCATION_LENGTH = 1024\n",
    "BATCH_SIZE = 64\n",
    "NUM_EVENTS_TRAIN =  50000\n",
    "NUM_EVENTS_VALID = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import collate_fn_with_class_loss, DatasetMode\n",
    "\n",
    "hits_norm = ConstraintsNormalizer()\n",
    "params_norm = TrackParamsNormalizer()\n",
    "train_data = SPDEventsDataset(\n",
    "    max_event_tracks=MAX_EVENT_TRACKS,\n",
    "    generate_fixed_tracks_num=False,\n",
    "    hits_normalizer=hits_norm,\n",
    "    track_params_normalizer=params_norm,\n",
    "    shuffle=True,\n",
    "    truncation_length=TRUNCATION_LENGTH,\n",
    "    n_samples=NUM_EVENTS_TRAIN,\n",
    "    mode=DatasetMode.train,\n",
    "    \n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_with_class_loss,\n",
    "    num_workers=4,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_data = SPDEventsDataset(\n",
    "    n_samples=NUM_EVENTS_VALID,\n",
    "    max_event_tracks=MAX_EVENT_TRACKS,\n",
    "    truncation_length=TRUNCATION_LENGTH,\n",
    "    generate_fixed_tracks_num=False,\n",
    "    hits_normalizer=hits_norm,\n",
    "    track_params_normalizer=params_norm,\n",
    "    mode=DatasetMode.val,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_with_class_loss,\n",
    "    num_workers=4,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4986, 0.5002, 0.5029, 0.2888, 0.3288, 0.8035, 1.0000],\n",
      "        [0.4986, 0.5002, 0.5029, 0.2582, 0.7055, 0.6422, 0.0000],\n",
      "        [0.4986, 0.5002, 0.5029, 0.5950, 0.2489, 0.1212, 1.0000],\n",
      "        [0.4986, 0.5002, 0.5029, 0.9962, 0.6036, 0.3745, 0.0000],\n",
      "        [0.4986, 0.5002, 0.5029, 0.5224, 0.5975, 0.8601, 0.0000],\n",
      "        [0.4986, 0.5002, 0.5029, 0.4812, 0.9956, 0.2096, 0.0000],\n",
      "        [0.4986, 0.5002, 0.5029, 0.7359, 0.3758, 0.1811, 1.0000],\n",
      "        [0.4986, 0.5002, 0.5029, 0.2623, 0.3439, 0.2612, 1.0000],\n",
      "        [0.4986, 0.5002, 0.5029, 0.6519, 0.3243, 0.5381, 1.0000],\n",
      "        [0.4986, 0.5002, 0.5029, 0.9035, 0.0878, 0.3956, 1.0000]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# read single batch for test\n",
    "for batch in train_loader:\n",
    "    print(batch[\"targets\"][0])\n",
    "    print(batch[\"labels\"][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for hungarian loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def match_targets(outputs, targets):\n",
    "    cost_matrix = torch.cdist(outputs, targets, p=1)\n",
    "    row_ind, col_ind = linear_sum_assignment(\n",
    "        cost_matrix.cpu().detach().numpy()\n",
    "    )\n",
    "    return row_ind, col_ind\n",
    "\n",
    "\n",
    "def hungarian_loss(outputs, targets):\n",
    "    row_ind, col_ind = match_targets(outputs, targets)\n",
    "    matched_outputs = outputs[row_ind]\n",
    "    matched_targets = targets[col_ind]\n",
    "    loss_dist = F.l1_loss(matched_outputs, matched_targets)\n",
    "    return loss_dist\n",
    "\n",
    "\n",
    "def criterion(preds, targets, preds_lengths, targets_lengths):\n",
    "    hungarian = torch.tensor(0.0)\n",
    "    for i in range(preds.shape[0]):\n",
    "        hungarian += hungarian_loss(\n",
    "            preds[i, :preds_lengths[i]],\n",
    "            targets[i, :targets_lengths[i]]\n",
    "        )\n",
    "    hungarian /= preds.shape[0]  # batchmean\n",
    "    return hungarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['inputs', 'mask', 'targets', 'orig_params', 'n_tracks_per_sample', 'labels'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"targets\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test same inputs, but shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Targets:\n",
      "tensor([1, 2, 1, 1, 0, 1, 1, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "def adjust_targets(row_ind, col_ind, targets, num_candidates=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: Predicted logits with shape [num_candidates, num_classes]\n",
    "        row_ind: Matched row indices for predictions N (for N matched pairs).\n",
    "        col_ind: Matched column indices for predictions N.\n",
    "        targets: Ground truth labels corresponding to matched pairs N.\n",
    "        num_candidates (int): Number of candidates predicted per sample (default=10).\n",
    "\n",
    "    Returns:\n",
    "        adjusted_logits: Logits with shape [num_candidates, num_classes], adjusted for unmatched candidates.\n",
    "        adjusted_targets: Target labels with shape num_candidates, where unmatched candidates get label 1.\n",
    "    \"\"\"\n",
    "    # Initialize adjusted logits and targets\n",
    "    #adjusted_logits = logits  #.clone()  # Copy logits\n",
    "    adjusted_targets = torch.ones(num_candidates, dtype=torch.long, device=targets.device)  # Default label is 1 for unmatched candidates\n",
    "\n",
    "    # For each matched pair, assign the corresponding target\n",
    "    matched_rows = row_ind\n",
    "    matched_cols = col_ind\n",
    "    adjusted_targets[matched_rows] = targets[matched_cols]\n",
    "\n",
    "    return adjusted_targets\n",
    "\n",
    "\n",
    "# Test\n",
    "num_candidates = 10\n",
    "num_classes = 5\n",
    "num_matched_pairs = 3\n",
    "\n",
    "# Logits: shape [10, C]\n",
    "logits = torch.randn(num_candidates, num_classes)\n",
    "\n",
    "# Random indices of matched pairs (row_ind and col_ind) for 3 matched elements per sample\n",
    "row_ind =  torch.tensor([1, 4, 7])\n",
    "col_ind = torch.tensor([0, 1, 2])\n",
    "\n",
    "# Targets for matched pairs, shape N\n",
    "targets =  torch.tensor([2, 0, 3])\n",
    "\n",
    "# Adjust logits and targets for unmatched predictions\n",
    "adjusted_targets = adjust_targets(row_ind, col_ind, targets, num_candidates)\n",
    "\n",
    "print(\"Adjusted Targets:\")\n",
    "print(adjusted_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare loss and overfit on a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def focal_loss(logits, targets, alpha=1, gamma=2, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: Predictions for each class with shape [B, N, C] where C is the number of classes (raw logits, not softmaxed).\n",
    "        targets: Ground truth labels with shape [B, N] where each value is in the range [0, C-1].\n",
    "        alpha (float, optional): A balancing factor for classes (default=1).\n",
    "        gamma (float, optional): Focusing parameter for hard examples (default=2).\n",
    "        reduction (string, optional): Specifies the reduction to apply to the output:\n",
    "                                      'none' | 'mean' | 'sum'. 'mean': the sum of the output will be divided by the number of elements in the output;\n",
    "                                      'sum': the output will be summed;\n",
    "                                      'none': no reduction will be applied (default='mean').\n",
    "    Returns:\n",
    "        Loss: Scalar if reduction is applied or the same shape as input without reduction.\n",
    "    \"\"\"\n",
    "    # Convert logits to probabilities with softmax\n",
    "    probs = F.softmax(logits, dim=-1)  # [N, C]\n",
    "\n",
    "    # Get the probabilities of the targets\n",
    "    targets = targets.unsqueeze(-1)  # [N, 1] to align with logits\n",
    "    probs_target_class = probs.gather(dim=-1, index=targets).squeeze(-1)  # [N]\n",
    "\n",
    "    # Compute the focal loss\n",
    "    log_pt = torch.log(probs_target_class + 1e-9)  # Stability for log\n",
    "    loss = -alpha * (1 - probs_target_class) ** gamma * log_pt  # Focal loss equation\n",
    "\n",
    "    # Apply the reduction\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        return loss  # No reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def match_targets(outputs, targets):\n",
    "    cost_matrix = torch.cdist(outputs, targets, p=1)\n",
    "    row_ind, col_ind = linear_sum_assignment(\n",
    "        cost_matrix.cpu().detach().numpy()\n",
    "    )\n",
    "    return row_ind, col_ind\n",
    "\n",
    "def hungarian_loss(outputs, targets, distance: Callable):\n",
    "\n",
    "    # loss = F.l1_loss(matched_outputs, matched_targets)\n",
    "    # loss = F.smooth_l1_loss(matched_outputs, matched_targets)\n",
    "    # loss = F.mse_loss(matched_outputs, matched_targets)\n",
    "    loss = distance(outputs, targets)\n",
    "    return loss\n",
    "\n",
    "def class_loss(outputs, targets, loss_fn: Callable):\n",
    "    return loss_fn(outputs, targets)\n",
    "\n",
    "\n",
    "class TRTHungarianLoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            distance: Callable = F.l1_loss,\n",
    "            class_loss: Callable = F.cross_entropy,\n",
    "            weights: tuple[float, float] = (1, 1)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._distance = distance\n",
    "        self._class_loss = class_loss\n",
    "        self._weights = weights\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            preds: dict[str, torch.Tensor],\n",
    "            targets: dict[str, torch.Tensor],\n",
    "            preds_lengths,\n",
    "            targets_lengths,\n",
    "    ):\n",
    "        batch_size = preds[\"params\"].shape[0]\n",
    "        pred_logits = preds[\"logits\"]\n",
    "        pred_params = preds[\"params\"]\n",
    "        target_params = targets[\"targets\"]\n",
    "        target_labels = targets[\"labels\"]\n",
    "        hungarian = torch.tensor(0.0).to(pred_params.device)\n",
    "        label_loss = torch.tensor(0.0).to(pred_params.device)\n",
    "        for i in range(batch_size):\n",
    "            row_ind, col_ind = match_targets(\n",
    "                pred_params[i, :preds_lengths[i]],\n",
    "                target_params[i, :targets_lengths[i]])\n",
    "            matched_outputs = pred_params[i, row_ind]\n",
    "            matched_targets = target_params[i, col_ind]\n",
    "            hungarian += hungarian_loss(\n",
    "                matched_outputs,\n",
    "                matched_targets,\n",
    "                distance=self._distance\n",
    "            )\n",
    "            matched_targets = adjust_targets(\n",
    "                row_ind,\n",
    "                col_ind,\n",
    "                target_labels[i, :targets_lengths[i]],\n",
    "                num_candidates=pred_logits.shape[1]\n",
    "            )\n",
    "            label_loss += class_loss(\n",
    "                pred_logits[i],\n",
    "                matched_targets,\n",
    "                loss_fn=self._class_loss\n",
    "            )\n",
    "        hungarian /= batch_size  # batchmean\n",
    "        label_loss /= batch_size  # batchmean\n",
    "        return self._weights[0] * hungarian + self._weights[1] * label_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "criterion = TRTHungarianLoss(class_loss=focal_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "model = TRT(\n",
    "    num_candidates=30,\n",
    "    n_points=TRUNCATION_LENGTH,\n",
    "    num_out_params=batch[\"targets\"].shape[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(batch[\"inputs\"], batch[\"mask\"])\n",
    "\n",
    "print(outputs[\"params\"].shape)\n",
    "print(outputs[\"logits\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'constants',\n",
       " 'data_generation',\n",
       " 'dataset',\n",
       " 'model',\n",
       " 'normalization',\n",
       " 'visualization']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import src\n",
    "dir(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.model_hybrid import TRTHybrid\n",
    "\n",
    "model_h = TRTHybrid(\n",
    "    num_candidates=30,\n",
    "    n_points=TRUNCATION_LENGTH,\n",
    "    num_out_params=batch[\"targets\"].shape[2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 30, 7])\n",
      "torch.Size([64, 30, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model_h(inputs=batch[\"inputs\"], mask=batch[\"mask\"])\n",
    "\n",
    "print(outputs[\"params\"].shape)\n",
    "print(outputs[\"logits\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor([MAX_EVENT_TRACKS]*len(outputs[\"params\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\projects'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Distance for Hungarian Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4e1511d5bb45f994842ad33e4cc71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 25528, 23036, 9724) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\trt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\trt\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m num_val_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     45\u001b[0m     num_val_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     46\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\trt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\trt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\trt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\trt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1144\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1143\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 25528, 23036, 9724) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available(\n",
    ") else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "model = TRTHybrid(\n",
    "    num_candidates=MAX_EVENT_TRACKS*5,\n",
    "    n_points=TRUNCATION_LENGTH,\n",
    "    num_out_params=batch[\"targets\"].shape[2]\n",
    ").to(device)\n",
    "criterion = TRTHungarianLoss(weights=(1, 0.02)).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "progress_bar = tqdm(range(5000))\n",
    "min_loss_train = min_loss_val = 1e5\n",
    "for epoch in progress_bar:        \n",
    "    train_loss = 0\n",
    "    num_train_batches = 0\n",
    "    for batch in train_loader:\n",
    "        num_train_batches += 1\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(batch[\"inputs\"].to(device), batch[\"mask\"].to(device))\n",
    "        loss = criterion(\n",
    "            preds=outputs,\n",
    "            targets={\n",
    "                \"targets\": batch[\"targets\"].to(device),\n",
    "                \"labels\": batch[\"labels\"].to(device),\n",
    "            },\n",
    "            preds_lengths=torch.LongTensor(\n",
    "                [MAX_EVENT_TRACKS]*len(outputs[\"params\"])\n",
    "            ).to(device),\n",
    "            targets_lengths=batch[\"n_tracks_per_sample\"].to(device)\n",
    "        )\n",
    "        train_loss += loss.detach().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if train_loss < min_loss_train:\n",
    "        min_loss = train_loss\n",
    "        torch.save(model.state_dict(), \"E:/projects/trt/weights/trt_hybrid_10_10_train.pt\")\n",
    "        \n",
    "    val_loss = 0\n",
    "    num_val_batches = 0\n",
    "    model.eval()\n",
    "    for batch in val_loader:\n",
    "        num_val_batches += 1\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(batch[\"inputs\"].to(device), batch[\"mask\"].to(device))\n",
    "        loss = criterion(\n",
    "            preds=outputs,\n",
    "            targets={\n",
    "                \"targets\": batch[\"targets\"].to(device),\n",
    "                \"labels\": batch[\"labels\"].to(device),\n",
    "            },\n",
    "            preds_lengths=torch.LongTensor(\n",
    "                [MAX_EVENT_TRACKS]*len(outputs[\"params\"])\n",
    "            ).to(device),\n",
    "            targets_lengths=batch[\"n_tracks_per_sample\"].to(device)\n",
    "        )\n",
    "        val_loss += loss.detach().item()\n",
    "        \n",
    "    if val_loss < min_loss_val:\n",
    "        min_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"E:/projects/trt/weights/trt_hybrid_10_10_val.pt\")\n",
    "        \n",
    "    progress_bar.set_postfix(\n",
    "        {\n",
    "            \"epoch\": epoch, \n",
    "            \"val_loss\": loss.detach().item()/num_val_batches,  \n",
    "            \"train_loss\": loss.detach().item()/num_train_batches}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"weights/trt_hybrid.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Smooth Distance for Hungarian Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available(\n",
    ") else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "model = TRT(\n",
    "    num_candidates=MAX_EVENT_TRACKS,\n",
    "    n_points=TRUNCATION_LENGTH,\n",
    "    num_out_params=batch[\"targets\"].shape[2]\n",
    ").to(device)\n",
    "criterion = TRTHungarianLoss(distance=F.smooth_l1_loss).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0003)\n",
    "progress_bar = tqdm(range(10000))\n",
    "for epoch in progress_bar:\n",
    "    train_loss = 0\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    outputs = model(batch[\"inputs\"].to(device), batch[\"mask\"].to(device))\n",
    "    loss = criterion(\n",
    "        preds=outputs[\"params\"],\n",
    "        targets=batch[\"targets\"].to(device),\n",
    "        preds_lengths=torch.LongTensor(\n",
    "            [MAX_EVENT_TRACKS]*len(outputs[\"params\"])\n",
    "        ).to(device),\n",
    "        targets_lengths=batch[\"n_tracks_per_sample\"].to(device)\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    progress_bar.set_postfix({\"epoch\": epoch, \"loss\": loss.detach().item()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE distance for Hungarian Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available(\n",
    ") else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "model = TRT(\n",
    "    num_candidates=MAX_EVENT_TRACKS,\n",
    "    n_points=TRUNCATION_LENGTH,\n",
    "    num_out_params=batch[\"targets\"].shape[2]\n",
    ").to(device)\n",
    "criterion = TRTHungarianLoss(distance=F.mse_loss).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0003)\n",
    "progress_bar = tqdm(range(10000))\n",
    "for epoch in progress_bar:\n",
    "    train_loss = 0\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    outputs = model(batch[\"inputs\"].to(device), batch[\"mask\"].to(device))\n",
    "    loss = criterion(\n",
    "        preds=outputs[\"params\"],\n",
    "        targets=batch[\"targets\"].to(device),\n",
    "        preds_lengths=torch.LongTensor(\n",
    "            [MAX_EVENT_TRACKS]*len(outputs[\"params\"])\n",
    "        ).to(device),\n",
    "        targets_lengths=batch[\"n_tracks_per_sample\"].to(device)\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    progress_bar.set_postfix({\"epoch\": epoch, \"loss\": loss.detach().item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
